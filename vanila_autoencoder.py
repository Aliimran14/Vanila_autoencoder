# -*- coding: utf-8 -*-
"""vanila autoencoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QGW7MgMjYkIunWE-43UTsCREOSa5W3Jw
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
# %pylab inline
from tensorflow import keras

# Load the MNIST dataset from Keras
(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()

# Normalize the training data to be in the range [0, 1]
x_train = x_train / 255.0

# Normalize the test data to be in the range [0, 1]
x_test = x_test / 255.0

# Create the encoder model
encoder = keras.models.Sequential([
    # Flatten the input images (28x28) into a 1D array of 784 elements
    keras.layers.Flatten(input_shape=[28, 28]),
    # Add a dense layer with 100 neurons and ReLU activation function
    keras.layers.Dense(100, activation="relu"),
    # Add another dense layer with 30 neurons and ReLU activation function
    keras.layers.Dense(30, activation="relu"),
])

# Create the decoder model
decoder = keras.models.Sequential([
    # Add a dense layer with 100 neurons and ReLU activation function, input shape is 30 (output from encoder)
    keras.layers.Dense(100, activation="relu", input_shape=[30]),
    # Add a dense layer with 784 neurons (28*28) and sigmoid activation function to output pixel values in range [0, 1]
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    # Reshape the output into the original image shape (28x28)
    keras.layers.Reshape([28, 28])
])

# Stack the encoder and decoder models to create the autoencoder
stacked_autoencoder = keras.models.Sequential([encoder, decoder])

# Compile the stacked autoencoder model
stacked_autoencoder.compile(
    # Use binary cross-entropy loss function for binary input data (normalized pixel values)
    loss="binary_crossentropy",
    # Use the Adam optimizer for efficient training
    optimizer='adam'
)

# Train the stacked autoencoder model
history = stacked_autoencoder.fit(
    # Input and target data are both x_train (input images) for reconstruction
    x_train, x_train,
    # Train for 10 epochs
    epochs=10,
    # Use x_test for validation during training
    validation_data=[x_test, x_test]
)

# Set the figure size for the plot
figsize(20, 5)

# Iterate over a range of 8 examples from the test set
for i in range(8):
    # Plot the original image from the test set
    subplot(2, 8, i+1)
    # Make a prediction using the stacked autoencoder on the current test image
    pred = stacked_autoencoder.predict(x_test[i].reshape((1, 28, 28)))
    # Display the original image
    imshow(x_test[i], cmap="binary")

    # Plot the reconstructed image by the stacked autoencoder
    subplot(2, 8, i+8+1)
    # Display the reconstructed image
    imshow(pred.reshape((28, 28)), cmap="binary")

# Set the index i to choose a specific example from the test set
i = 0  # Change this number to select a different example

# Set the figure size for the plot
figsize(10, 5)

# Plot the original image
subplot(1, 3, 1)
imshow(x_test[i], cmap="binary")

# Plot the latent vector representation obtained from the encoder
subplot(1, 3, 2)
# Predict the latent vector representation of the selected test image
latent_vector = encoder.predict(x_test[i].reshape((1, 28, 28)))
imshow(latent_vector, cmap="binary")

# Plot the reconstructed image from the latent vector using the decoder
subplot(1, 3, 3)
# Reconstruct the image from the latent vector representation
pred = decoder.predict(latent_vector)
imshow(pred.reshape((28, 28)), cmap="binary")

# 30 / (28 * 28), 1 - 30 / (28 * 28)

# Calculate the sparsity constraints
sparsity_low = 30 / (28 * 28)  # Lower bound for sparsity constraint
sparsity_high = 1 - 30 / (28 * 28)  # Upper bound for sparsity constraint

# Print the calculated values
print(sparsity_low, sparsity_high)